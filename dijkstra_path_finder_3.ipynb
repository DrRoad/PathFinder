{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N.B.\n",
    "\n",
    "After this notebook, we will switch back to Julia language.\n",
    "The reasons are threefold:\n",
    "\n",
    "* Julia provides a decent performance boost for free;\n",
    "* Julia is the main language at LakeTide, technical support has neither distance nor latency;\n",
    "* Julia has better support for OpenStreetMap, which is needed for future traffic simulation.\n",
    "\n",
    "Switching from Julia to Python was not a bad idea per se;\n",
    "Python plus Networkx worked quite well in the last two weeks as\n",
    "the tool set to conduct the proof of concepts; currently, new\n",
    "needs demand new set of tools."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Heads-Up\n",
    "\n",
    "In this notebook, we scale up the graph to 1000 nodes;\n",
    "this makes the size of samples around one billion, which\n",
    "makes it impossible to be generated and further augmented.\n",
    "\n",
    "The way to cope with that is to randomly sample pairs of nodes\n",
    "and generate training data from them. Idealy, the training size\n",
    "before augmentation is around one millon.\n",
    "\n",
    "The meaning of this stocastic manner is twofold.\n",
    "* it reduced the complexity of training set generation massively\n",
    "* this is a decent test to see if the model is truly able to generalize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problems\n",
    "\n",
    "The algorithm does not seem to work well with a graph\n",
    "of 1000 nodes, which is somehow disappointing.\n",
    "\n",
    "Possible reasons are the following:\n",
    "\n",
    "* The model is not powerful enough to learn the graph;\n",
    "* The stochastic sampling of training data does not show the model the big picture, i.e.,\n",
    "the model gets confused by the edges it has never seen before;\n",
    "\n",
    "* The training sample should allow the model the take steps back, i.e., for each sample\n",
    "path, the parent nodes should be included;\n",
    "\n",
    "However, after deeper analysis, we tend to believe that it is certain properties of \n",
    "the graph that make the current model much harder to be trained and to infer. (\"./Mar_19/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make it \"smarter\" TODO\n",
    "\n",
    "Dead loop detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import numpy as np\n",
    "import mxnet as mx\n",
    "import logging\n",
    "from sklearn.metrics import accuracy_score\n",
    "from utils import plot_g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_NODE = 100\n",
    "WEIGHT_MIN = .5\n",
    "WEIGHT_MAX = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_euclidean_dist(G, tmp_node, node):\n",
    "    \n",
    "    p_1 = np.array([G.nodes[tmp_node]['x'], G.nodes[tmp_node]['y']])\n",
    "    p_2 = np.array([G.nodes[node]['x'], G.nodes[node]['y']])\n",
    "    return np.sqrt(np.sum((p_1 - p_2)**2))\n",
    "\n",
    "def generate_low_degree_g(num_nodes=100, min_out_degree=2, max_out_degree=4, weight_min=WEIGHT_MIN, weight_max=WEIGHT_MAX):\n",
    "    \n",
    "    G = nx.Graph()\n",
    "    \n",
    "    grid_size = 20000\n",
    "    euclidean_coords = np.linspace(0.0, 1.0, num=grid_size, endpoint=False)\n",
    "    coords_indices = list(range(grid_size))\n",
    "    \n",
    "    random.shuffle(coords_indices)\n",
    "    x_coords = euclidean_coords[coords_indices][:num_nodes]\n",
    "#     print(x_coords)\n",
    "    random.shuffle(coords_indices)\n",
    "    y_coords = euclidean_coords[coords_indices][:num_nodes]\n",
    "#     print(y_coords)\n",
    "    \n",
    "    # Add coordinates to nodes\n",
    "    for node, coord in enumerate(zip(x_coords, y_coords)):\n",
    "#         print(node, coord[0], coord[1])\n",
    "        G.add_node(node, x=coord[0], y=coord[1])\n",
    "    \n",
    "    for node in G.nodes:\n",
    "        \n",
    "        tmp_nodes = list(G.nodes)\n",
    "        tmp_nodes.remove(node)\n",
    "        node_dist = map(lambda tmp_node: (tmp_node, calc_euclidean_dist(G, tmp_node, node)), tmp_nodes)\n",
    "        node_dist = sorted(node_dist, key=lambda d:d[1])\n",
    "        \n",
    "        num_of_neighbors = random.randint(min_out_degree, max_out_degree)\n",
    "#         print(node, out_neighbors)\n",
    "        \n",
    "#         G.add_edges_from(map(lambda d:(node, d), out_neighbors))\n",
    "        \n",
    "        for tmp_node in node_dist:\n",
    "\n",
    "            if G.degree(tmp_node[0]) >= max_out_degree \\\n",
    "                or G.degree(node) >= num_of_neighbors:\n",
    "                # This node has maximum number of neighbors already\n",
    "                continue\n",
    "            \n",
    "            weight = random.uniform(weight_min, weight_max)\n",
    "            geo_dist = calc_euclidean_dist(G, tmp_node[0], node)\n",
    "            \n",
    "            G.add_edge(node, tmp_node[0], weight=weight * geo_dist)\n",
    "    \n",
    "    # Add centrality to edges\n",
    "    edge_centrality = nx.edge_betweenness_centrality(G, \\\n",
    "                                                     k=G.number_of_nodes(), \\\n",
    "                                                     weight='weight')\n",
    "    assert len(edge_centrality) == G.number_of_edges()\n",
    "    \n",
    "    for edge_data in G.edges.data():\n",
    "        edge_data[2]['centrality'] = edge_centrality[(edge_data[0], edge_data[1])]\n",
    "        \n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# G = generate_low_degree_g(num_nodes=1000)\n",
    "# max_degree = max(G.degree, key=lambda d: d[1])[1]\n",
    "# min_degree = min(G.degree, key=lambda d: d[1])[1]\n",
    "# print(max_degree)\n",
    "# print(min_degree)\n",
    "# print(G.number_of_nodes())\n",
    "# print(G.number_of_edges())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.read_gpickle(\"./Mar_19/graph.pickle\")\n",
    "max_degree = max(G.degree, key=lambda d: d[1])[1]\n",
    "min_degree = min(G.degree, key=lambda d: d[1])[1]\n",
    "print(max_degree)\n",
    "print(min_degree)\n",
    "print(G.number_of_nodes())\n",
    "print(G.number_of_edges())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_g(G, with_labels=False, node_size=20, font_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from networkx import NetworkXNoPath\n",
    "\n",
    "def calc_cosine_dist(p1, p2):\n",
    "    return np.dot(p1, p2) / (np.linalg.norm(p1) * np.linalg.norm(p2))\n",
    "\n",
    "def encode_edges(G, parent, node, src, dst):\n",
    "    \n",
    "    ret = np.zeros((max_degree, 7))\n",
    "    ret[:, -1] = -1\n",
    "    \n",
    "    x_dst = G.nodes[dst]['x']\n",
    "    y_dst = G.nodes[dst]['y']\n",
    "    x_src = G.nodes[src]['x']\n",
    "    y_src = G.nodes[src]['y']\n",
    "    \n",
    "    for idx, edge in enumerate(G.edges(node)):\n",
    "        u = edge[0]\n",
    "        v = edge[1]\n",
    "        \n",
    "        # Only look forward\n",
    "        # TODO: allow look backwards\n",
    "#         if v == parent:\n",
    "#             continue\n",
    "        \n",
    "        ret[idx][0] = G.get_edge_data(u, v)['centrality']\n",
    "        ret[idx][1] = G.get_edge_data(u, v)['weight']\n",
    "        \n",
    "        x_u = G.nodes[u]['x']\n",
    "        y_u = G.nodes[u]['y']\n",
    "        x_v = G.nodes[v]['x']\n",
    "        y_v = G.nodes[v]['y']\n",
    "\n",
    "        ret[idx][2] = calc_cosine_dist((x_v-x_u, y_v-y_u), (x_dst-x_u, y_dst-y_u))\n",
    "        ret[idx][3] = calc_euclidean_dist(G, v, dst)\n",
    "        ret[idx][4] = x_v\n",
    "        ret[idx][5] = y_v\n",
    "        ret[idx][-1] = v\n",
    "        \n",
    "    return (ret, x_src, y_src, x_dst, y_dst)\n",
    "\n",
    "def generate_stochastic_dataset(G, sample_size_lower_bound=500, frequent=100):\n",
    "    \n",
    "    X = []\n",
    "    y = []\n",
    "    \n",
    "    pair_path_dict = dict()\n",
    "    \n",
    "    selected_pairs = []\n",
    "    sample_cnt = 0\n",
    "    num_of_nodes = G.number_of_nodes()\n",
    "    \n",
    "    while True:\n",
    "        src = np.random.randint(0, num_of_nodes)\n",
    "        dst = np.random.randint(0, num_of_nodes)\n",
    "        \n",
    "        if src == dst:\n",
    "            continue\n",
    "        \n",
    "        selected_pairs.append((src, dst))\n",
    "        \n",
    "        for src, dst in [(src, dst), (dst, src)]:\n",
    "            try:\n",
    "                path = nx.dijkstra_path(G, src, dst)\n",
    "                # TODO record all paths for later use\n",
    "                \n",
    "            except NetworkXNoPath:\n",
    "                print(\"No path between %d and %d\" % (src, dst))\n",
    "                break\n",
    "            \n",
    "            pair_path_dict[(src, dst)] = path\n",
    "            \n",
    "            parent_node = -1\n",
    "            cur_node = src\n",
    "            for mid_node in path[1:]:\n",
    "\n",
    "#                 print('parent:(%d), X:(%d, %d), y:(%d)' % (parent_node, cur_node, dst, mid_node))\n",
    "                X.append(encode_edges(G, parent_node, cur_node, src, dst))\n",
    "                y.append(mid_node)\n",
    "                \n",
    "                parent_node = cur_node\n",
    "                cur_node = mid_node\n",
    "                \n",
    "                sample_cnt += 1\n",
    "                \n",
    "                if sample_cnt % frequent == 0:\n",
    "                    print(\"Collected %d samples.\" % (sample_cnt))\n",
    "        \n",
    "        if sample_cnt >= sample_size_lower_bound:\n",
    "            break\n",
    "        \n",
    "    return X, y, selected_pairs, pair_path_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "features, labels, selected_pairs, pair_path_dict = generate_stochastic_dataset(G, \\\n",
    "                                                               sample_size_lower_bound=250000, \\\n",
    "                                                               frequent=30000)\n",
    "print(\"%d pairs are selected\" % (len(selected_pairs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(labels))\n",
    "print(len(selected_pairs))\n",
    "print(len(pair_path_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pair_path_len_dict = map(lambda d: (d[0], len(d[1])), pair_path_dict.items())\n",
    "max_selected_path_len = max(pair_path_len_dict, key=lambda d:d[1])\n",
    "min_selected_path_len = min(pair_path_len_dict, key=lambda d:d[1])\n",
    "\n",
    "print max_selected_path_len\n",
    "print min_selected_path_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_label_idx(feature, label):\n",
    "    \n",
    "    for idx, row in enumerate(feature):\n",
    "        if label == row[-1]:\n",
    "            break\n",
    "    \n",
    "    return idx\n",
    "\n",
    "def one_hot_encode(idx, length=max_degree):\n",
    "    \n",
    "    ret = np.zeros(length)\n",
    "    ret[idx] = 1.0\n",
    "    return ret\n",
    "\n",
    "def augment_dataset(X, y, augmentation_index=20, print_freq=300000):\n",
    "    \n",
    "    sample_size = len(y)\n",
    "    augmented_sample_size = augmentation_index * sample_size\n",
    "    feature_size = 28\n",
    "    \n",
    "    X_aug, y_aug = np.zeros((augmented_sample_size, feature_size)), np.zeros((augmented_sample_size), dtype=np.int)\n",
    "    indices = np.arange(max_degree)\n",
    "    \n",
    "    cnt = 0\n",
    "    for feature, label in zip(X, y):\n",
    "        for _ in range(augmentation_index):\n",
    "            np.random.shuffle(indices)\n",
    "            \n",
    "            feature_tmp = feature[0][indices]\n",
    "            label_tmp = find_label_idx(feature_tmp, label)\n",
    "            \n",
    "            X_aug[cnt] = np.append(feature_tmp[:, :-1].reshape(-1), [feature[1], feature[2], feature[3], feature[4]])\n",
    "            y_aug[cnt] = label_tmp\n",
    "            \n",
    "            cnt += 1\n",
    "            \n",
    "            if cnt % print_freq == 0:\n",
    "                print(\"Processed %d samples\" % (cnt))\n",
    "            \n",
    "    return X_aug, y_aug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X, y = augment_dataset(features, labels,\\\n",
    "                       augmentation_index=20, \\\n",
    "                       print_freq=600000)\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, test_size=.2, random_state=42)\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "train_iter = mx.io.NDArrayIter(X_train, y_train, batch_size, shuffle=True)\n",
    "val_iter = mx.io.NDArrayIter(X_test, y_test, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(context=mx.gpu()):\n",
    "    \n",
    "    data = mx.sym.var('data')\n",
    "    label = mx.sym.var('softmax_label')\n",
    "    \n",
    "    fc1  = mx.sym.FullyConnected(data=data, num_hidden=1024)\n",
    "    fc1 = mx.sym.Activation(data=fc1, act_type=\"relu\")\n",
    "    \n",
    "    fc2  = mx.sym.FullyConnected(data=fc1, num_hidden=512)\n",
    "    fc2 = mx.sym.Activation(data=fc2, act_type=\"relu\")\n",
    "    \n",
    "    fc3  = mx.sym.FullyConnected(data=fc2, num_hidden=max_degree)\n",
    "    mlp  = mx.sym.SoftmaxOutput(data=fc3, label=label)\n",
    "    \n",
    "    return mx.mod.Module(symbol=mlp, context=context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.getLogger().setLevel(logging.DEBUG)  # logging to stdout\n",
    "model = build_model()\n",
    "model.bind(data_shapes=train_iter.provide_data, label_shapes=train_iter.provide_label)\n",
    "model.init_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.fit(train_iter,  # train data\n",
    "              eval_data=val_iter,  # validation data\n",
    "              optimizer='adam',  # use SGD to train\n",
    "#               optimizer_params={'learning_rate':0.01, 'momentum': 0.9},\n",
    "              eval_metric='acc',  # report accuracy during training\n",
    "              batch_end_callback = mx.callback.Speedometer(batch_size, 5000),\n",
    "              epoch_end_callback = mx.callback.do_checkpoint(\"mymodel\", 1),\n",
    "              num_epoch=10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# acc = mx.metric.Accuracy()\n",
    "# train_iter = mx.io.NDArrayIter(X_train, y_train, batch_size=batch_size)\n",
    "# model.score(train_iter, acc)\n",
    "# print('Acc on training set %f' % acc.get()[1])\n",
    "\n",
    "# acc = mx.metric.Accuracy()\n",
    "# test_iter = mx.io.NDArrayIter(X_test, y_test, batch_size=batch_size)\n",
    "# model.score(test_iter, acc)\n",
    "# print('Acc on test set %f' % acc.get()[1])\n",
    "\n",
    "acc = mx.metric.Accuracy()\n",
    "all_iter = mx.io.NDArrayIter(X, y, batch_size=batch_size)\n",
    "model.score(all_iter, acc)\n",
    "print('Acc on all %f' % acc.get()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_infer_next_node(G, path, parent, cur_node, src, dst, model):\n",
    "#     print(\"In nn_infer_next_node\")\n",
    "    input_vec, x_src, y_src, x_dst, y_dst = encode_edges(G, parent, cur_node, src, dst)\n",
    "    \n",
    "    real_input_vec = np.append(input_vec[:, :-1].reshape(-1), [x_src, y_src, x_dst, y_dst])\n",
    "    pred = model.predict(mx.io.NDArrayIter(np.array([real_input_vec]), np.array([0]))).asnumpy()[0]\n",
    "    pred_idx = np.argmax(pred)\n",
    "    neighbor = int(input_vec[pred_idx][-1])\n",
    "    \n",
    "    if neighbor == -1:\n",
    "        print(\"Invalid prediction, randomizing next node\")\n",
    "        avaliable_neighbors = filter(lambda d: d >= 0, input_vec[:, -1])\n",
    "        neighbor = int(avaliable_neighbors[np.random.randint(0, len(avaliable_neighbors))])\n",
    "    \n",
    "#     if neighbor in path:\n",
    "#         print(\"Duplicated nodes, randomizing next node\")\n",
    "#         avaliable_neighbors = filter(lambda d: d >= 0 and d != neighbor, input_vec[:, -1])\n",
    "\n",
    "#         if len(avaliable_neighbors) == 0:\n",
    "#             print(\"Duplicated nodes, no other nodes available\")\n",
    "#         else:\n",
    "#             neighbor = int(avaliable_neighbors[np.random.randint(0, len(avaliable_neighbors))])\n",
    "        \n",
    "    return (neighbor, G.get_edge_data(cur_node, neighbor)['weight'])\n",
    "\n",
    "def dijkstra_path_finder(G, src, dst, model=None, invalid_path_threshold=100):\n",
    "    \n",
    "    path = [src]\n",
    "    parent_node = -1\n",
    "    cur_node = src\n",
    "    total_weights = .0\n",
    "    \n",
    "    while True:\n",
    "        \n",
    "        if len(path) >= invalid_path_threshold:\n",
    "            return path, total_weights, False\n",
    "        \n",
    "        next_node, weight = nn_infer_next_node(G, path, parent_node, cur_node, src, dst, model)\n",
    "        total_weights += weight\n",
    "        path.append(next_node)\n",
    "        \n",
    "        if next_node == dst:\n",
    "            return path, total_weights, True\n",
    "        \n",
    "        parent_node = cur_node\n",
    "        cur_node = next_node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# G = generate_low_degree_g()\n",
    "def calc_statistics(G, pair_path_dict, num_to_test=100):\n",
    "    found_cnt = 0\n",
    "    opt_path_cnt = 0\n",
    "    \n",
    "    def calc_dot_dist(p1, p2):\n",
    "        return np.dot(p1, p2)\n",
    "\n",
    "    def calc_geo_dist(p1, p2):\n",
    "\n",
    "        p_1 = np.array(p1)\n",
    "        p_2 = np.array(p2)\n",
    "\n",
    "        return np.sqrt(np.sum((p_1 - p_2)**2))\n",
    "\n",
    "    def heuristic(node, dst):\n",
    "\n",
    "        x_src = G.nodes[src]['x']\n",
    "        y_src = G.nodes[src]['y']\n",
    "        x_dst = G.nodes[dst]['x']\n",
    "        y_dst = G.nodes[dst]['y']\n",
    "        x_node = G.nodes[node]['x']\n",
    "        y_node = G.nodes[node]['y']\n",
    "\n",
    "        dist_src_2_dst = calc_geo_dist((x_src, y_src), (x_dst, y_dst))\n",
    "        dot_src_2_node = calc_dot_dist((x_node-x_src, y_node-y_src), (x_dst-x_src, y_dst-y_src))\n",
    "        ret = dist_src_2_dst - (dot_src_2_node) / dist_src_2_dst\n",
    "\n",
    "        return ret\n",
    "    \n",
    "    pairs = pair_path_dict.keys()\n",
    "    random.shuffle(pairs)\n",
    "    \n",
    "    for src, dst in pairs[:num_to_test]:\n",
    "\n",
    "        nn_path, _, found = dijkstra_path_finder(G, src, dst, model=model, invalid_path_threshold=G.number_of_nodes())\n",
    "        \n",
    "        dijkstra_path = pair_path_dict[(src, dst)]\n",
    "        \n",
    "        if not found:\n",
    "            \n",
    "            print(\"The model is unable to find a path between %d and %d\" % (src, dst))\n",
    "            print(\"Dijkstra path :\", dijkstra_path)\n",
    "            print(\"NN path :\", nn_path)\n",
    "            print('')\n",
    "            continue\n",
    "\n",
    "        found_cnt += 1\n",
    "        \n",
    "        if nn_path == dijkstra_path:\n",
    "            opt_path_cnt = opt_path_cnt + 1\n",
    "\n",
    "        else:\n",
    "            pass\n",
    "# #             astar_path = nx.astar_path(G, src, dst, heuristic=heuristic)\n",
    "#             print('Printing all paths for reference:')\n",
    "#             print(\"Dijkstra path :\", dijkstra_path)\n",
    "# #             print(\"A star path :\", astar_path)\n",
    "#             print(\"NN path :\", nn_path)\n",
    "#             print('')\n",
    "\n",
    "    return found_cnt, opt_path_cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_paths_statistics(G, pair_path_dict, num_to_test=100):\n",
    "\n",
    "    num_total_path = num_to_test\n",
    "\n",
    "    found_cnt, opt_path_cnt = calc_statistics(G, pair_path_dict=pair_path_dict, num_to_test=num_to_test)\n",
    "    print('%d out of %d can find path: %f' % (found_cnt, num_total_path, float(found_cnt)/num_total_path))\n",
    "    print('%d out of %d can find optimal path: %f' % (opt_path_cnt, found_cnt, float(opt_path_cnt)/found_cnt))\n",
    "    print('%d out of %d all paths can find optimal paths: %f' % (opt_path_cnt, num_total_path, float(opt_path_cnt)/num_total_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print_paths_statistics(G, num_to_test=100, pair_path_dict=pair_path_dict)\n",
    "\n",
    "# 9893 out of 9900 can find path: 0.999293\n",
    "# 8803 out of 9893 can find optimal path: 0.889821\n",
    "# 8803 out of 9900 all paths can find optimal paths: 0.889192"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dijkstra_node_list = [719, 775, 797, 564, 138, 703, 471, 882, 936, 888, 953, 566, 235, 826, 615, 708, 245, 724, 536, 486, 394, 403, 636, 937, 641, 664, 391, 384, 338, 168, 515, 334, 372, 354, 965, 856, 167, 606, 943, 782, 576]\n",
    "nn_node_list =       [719, 775, 797, 564, 138, 703, 471, 882, 936, 888, 953, 566, 235, 826, 615, 708, 245, 724, 536, 486, 394, 403, 636, 937, 641, 664, 438, 66, 819, 459, 640, 850, 269, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687, 425, 604, 687]\n",
    "pos = {}\n",
    "\n",
    "for node in dijkstra_node_list:\n",
    "    pos[node] = (G.nodes[node]['x'], G.nodes[node]['y'])\n",
    "\n",
    "dijkstra_edge_list = []\n",
    "for idx, node in enumerate(dijkstra_node_list[:-1]):\n",
    "    dijkstra_edge_list.append((node, dijkstra_node_list[idx+1]))\n",
    "\n",
    "nx.draw_networkx_edges(G, pos=pos, edgelist=dijkstra_edge_list, node_size=20, edge_color='r')\n",
    "\n",
    "nn_pos = {}\n",
    "\n",
    "for node in nn_node_list:\n",
    "    nn_pos[node] = (G.nodes[node]['x'], G.nodes[node]['y'])\n",
    "\n",
    "nn_edge_list = []\n",
    "for idx, node in enumerate(nn_node_list[:-1]):\n",
    "    nn_edge_list.append((node, nn_node_list[idx+1]))\n",
    "\n",
    "nx.draw_networkx_edges(G, pos=nn_pos, edgelist=set(nn_edge_list), node_size=100, edge_color='b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
