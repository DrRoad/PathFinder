{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N.B.\n",
    "\n",
    "After this notebook, we will switch back to Julia language.\n",
    "The reasons are threefold:\n",
    "\n",
    "* Julia provides a decent performance boost for free;\n",
    "* Julia is the main language at LakeTide, technical support has neither distance nor latency;\n",
    "* Julia has better support for OpenStreetMap, which is needed for future traffic simulation.\n",
    "\n",
    "Switching from Julia to Python was not a bad idea per se;\n",
    "Python plus Networkx worked quite well in the last two weeks as\n",
    "the tool set to conduct the proof of concepts; currently, new\n",
    "needs demand new set of tools."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Heads-Up\n",
    "\n",
    "In this notebook, we scale up the graph to 1000 nodes;\n",
    "this makes the size of samples around one billion, which is\n",
    "unrealistic to generate and further augment.\n",
    "\n",
    "The way to cope with that is to randomly sample pairs of nodes\n",
    "and generate training data from them. Idealy, the training size\n",
    "before augmentation is around one millon.\n",
    "\n",
    "The meaning of this stocastic manner is twofold.\n",
    "* it reduced the complexity of training set generation massively\n",
    "* this is a decent test to see if the model is truly able to generalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import numpy as np\n",
    "import mxnet as mx\n",
    "import logging\n",
    "from sklearn.metrics import accuracy_score\n",
    "from utils import plot_g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_NODE = 100\n",
    "WEIGHT_MIN = .5\n",
    "WEIGHT_MAX = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_euclidean_dist(G, tmp_node, node):\n",
    "    \n",
    "    p_1 = np.array([G.nodes[tmp_node]['x'], G.nodes[tmp_node]['y']])\n",
    "    p_2 = np.array([G.nodes[node]['x'], G.nodes[node]['y']])\n",
    "    return np.sqrt(np.sum((p_1 - p_2)**2))\n",
    "\n",
    "def generate_low_degree_g(node_size=100, min_out_degree=2, max_out_degree=4, weight_min=WEIGHT_MIN, weight_max=WEIGHT_MAX):\n",
    "    \n",
    "    G = nx.Graph()\n",
    "    \n",
    "    grid_size = 1000\n",
    "    euclidean_coords = np.linspace(0.01, 1.0, num=grid_size, endpoint=False)\n",
    "    coords_indices = list(range(grid_size))\n",
    "    \n",
    "    random.shuffle(coords_indices)\n",
    "    x_coords = euclidean_coords[coords_indices][:node_size]\n",
    "#     print(x_coords)\n",
    "    random.shuffle(coords_indices)\n",
    "    y_coords = euclidean_coords[coords_indices][:node_size]\n",
    "#     print(y_coords)\n",
    "    \n",
    "    # Add coordinates to nodes\n",
    "    for node, coord in enumerate(zip(x_coords, y_coords)):\n",
    "#         print(node, coord[0], coord[1])\n",
    "        G.add_node(node, x=coord[0], y=coord[1])\n",
    "    \n",
    "    for node in G.nodes:\n",
    "        \n",
    "        tmp_nodes = list(G.nodes)\n",
    "        tmp_nodes.remove(node)\n",
    "        node_dist = map(lambda tmp_node: (tmp_node, calc_euclidean_dist(G, tmp_node, node)), tmp_nodes)\n",
    "        node_dist = sorted(node_dist, key=lambda d:d[1])\n",
    "        \n",
    "        num_of_neighbors = random.randint(min_out_degree, max_out_degree)\n",
    "#         print(node, out_neighbors)\n",
    "        \n",
    "#         G.add_edges_from(map(lambda d:(node, d), out_neighbors))\n",
    "        \n",
    "        for tmp_node in node_dist:\n",
    "\n",
    "            if G.degree(tmp_node[0]) >= max_out_degree \\\n",
    "                or G.degree(node) >= num_of_neighbors:\n",
    "                # This node has maximum number of neighbors already\n",
    "                continue\n",
    "            \n",
    "            weight = random.uniform(weight_min, weight_max)\n",
    "            geo_dist = calc_euclidean_dist(G, tmp_node[0], node)\n",
    "            \n",
    "            G.add_edge(node, tmp_node[0], weight=weight * geo_dist)\n",
    "    \n",
    "    # Add centrality to edges\n",
    "    edge_centrality = nx.edge_betweenness_centrality(G, \\\n",
    "                                                     k=G.number_of_nodes(), \\\n",
    "                                                     weight='weight')\n",
    "    assert len(edge_centrality) == G.number_of_edges()\n",
    "    \n",
    "    for edge_data in G.edges.data():\n",
    "        edge_data[2]['centrality'] = edge_centrality[(edge_data[0], edge_data[1])]\n",
    "        \n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = generate_low_degree_g(node_size=20)\n",
    "max_degree = max(G.degree, key=lambda d: d[1])[1]\n",
    "min_degree = min(G.degree, key=lambda d: d[1])[1]\n",
    "print(max_degree)\n",
    "print(min_degree)\n",
    "print(G.number_of_nodes())\n",
    "print(G.number_of_edges())\n",
    "plot_g(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
