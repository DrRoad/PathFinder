{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Heads-Up\n",
    "\n",
    "In the notebook, we are working on generating\n",
    "graphs whose nodes have Euclidean coordinates;\n",
    "One thing to pay attention to is, in real world\n",
    "road network, nodes (intersections) that are close\n",
    "to each other geographically are more likely to be\n",
    "connected with edges (roads).\n",
    "\n",
    "Edge centrality will be added as part of the heuristic;\n",
    "meantime, cosine distance can also be a good heuristic;\n",
    "The geographical coordinates of nodes can be included."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions\n",
    "\n",
    "Does centrality really encode information about your destination?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Think with Analogy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import numpy as np\n",
    "import mxnet as mx\n",
    "import logging\n",
    "from sklearn.metrics import accuracy_score\n",
    "from utils import plot_g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "NUM_NODE = 100\n",
    "WEIGHT_MIN = .0\n",
    "WEIGHT_MAX = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def calc_euclidean_dist(G, tmp_node, node):\n",
    "    \n",
    "    p_1 = np.array([G.nodes[tmp_node]['x'], G.nodes[tmp_node]['y']])\n",
    "    p_2 = np.array([G.nodes[node]['x'], G.nodes[node]['y']])\n",
    "    return np.sqrt(np.sum((p_1 - p_2)**2))\n",
    "\n",
    "def generate_low_degree_g(node_size=20, min_out_degree=2, max_out_degree=4, weight_min=WEIGHT_MIN, weight_max=WEIGHT_MAX):\n",
    "    \n",
    "    G = nx.Graph()\n",
    "    \n",
    "    grid_size = 1000\n",
    "    euclidean_coords = np.linspace(0.01, 1.0, num=grid_size, endpoint=False)\n",
    "    coords_indices = list(range(grid_size))\n",
    "    \n",
    "    random.shuffle(coords_indices)\n",
    "    x_coords = euclidean_coords[coords_indices][:node_size]\n",
    "#     print(x_coords)\n",
    "    random.shuffle(coords_indices)\n",
    "    y_coords = euclidean_coords[coords_indices][:node_size]\n",
    "#     print(y_coords)\n",
    "    \n",
    "    # Add coordinates to nodes\n",
    "    for node, coord in enumerate(zip(x_coords, y_coords)):\n",
    "#         print(node, coord[0], coord[1])\n",
    "        G.add_node(node, x=coord[0], y=coord[1])\n",
    "    \n",
    "    for node in G.nodes:\n",
    "        \n",
    "        tmp_nodes = list(G.nodes)\n",
    "        tmp_nodes.remove(node)\n",
    "        node_dist = map(lambda tmp_node: (tmp_node, calc_euclidean_dist(G, tmp_node, node)), tmp_nodes)\n",
    "        node_dist = sorted(node_dist, key=lambda d:d[1])\n",
    "        \n",
    "        num_of_neighbors = random.randint(min_out_degree, max_out_degree)\n",
    "#         print(node, out_neighbors)\n",
    "        \n",
    "#         G.add_edges_from(map(lambda d:(node, d), out_neighbors))\n",
    "        \n",
    "        for tmp_node in node_dist:\n",
    "\n",
    "            if G.degree(tmp_node[0]) >= max_out_degree \\\n",
    "                or G.degree(node) >= num_of_neighbors:\n",
    "                # This node has maximum number of neighbors already\n",
    "                continue\n",
    "            \n",
    "            weight = random.uniform(weight_min, weight_max)\n",
    "            G.add_edge(node, tmp_node[0], weight=weight)\n",
    "    \n",
    "    # Add centrality to edges\n",
    "    edge_centrality = nx.edge_betweenness_centrality(G, \\\n",
    "                                                     k=G.number_of_nodes(), \\\n",
    "                                                     weight='weight')\n",
    "    assert len(edge_centrality) == G.number_of_edges()\n",
    "    \n",
    "    for edge_data in G.edges.data():\n",
    "        edge_data[2]['centrality'] = edge_centrality[(edge_data[0], edge_data[1])]\n",
    "        \n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "G = generate_low_degree_g()\n",
    "max_degree = max(G.degree, key=lambda d: d[1])[1]\n",
    "min_degree = min(G.degree, key=lambda d: d[1])[1]\n",
    "print(max_degree)\n",
    "print(min_degree)\n",
    "print(G.number_of_nodes())\n",
    "print(G.number_of_edges())\n",
    "plot_g(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def extract_path(prev, src, dst):\n",
    "    \n",
    "    path = []\n",
    "    u = dst\n",
    "\n",
    "    while prev[u] != src:\n",
    "        path.insert(0, u)\n",
    "        u = prev[u]\n",
    "    \n",
    "    path.insert(0, u)\n",
    "    path.insert(0, src)\n",
    "    return path\n",
    "\n",
    "def bfs_beam_edges_path(G, src, dst):\n",
    "    \n",
    "    centrality = nx.edge_betweenness_centrality(G, weight='weight', \\\n",
    "                                              k=G.number_of_nodes(), \\\n",
    "                                            normalized=True)\n",
    "    \n",
    "    prev = [-1 for _ in range(G.number_of_nodes())]\n",
    "    \n",
    "    for u, v in nx.bfs_beam_edges(G, src, value=centrality.get):\n",
    "        print(u, v, G.get_edge_data(u, v)['centrality'], G.get_edge_data(u, v)['weight'])\n",
    "        prev[v] = u\n",
    "\n",
    "    return extract_path(prev, src, dst)\n",
    "\n",
    "for src in G.nodes():\n",
    "    for dst in G.nodes():\n",
    "        \n",
    "        if src == dst:\n",
    "            continue\n",
    "        \n",
    "        print(nx.dijkstra_path(G, src, dst, weight='weight'))\n",
    "        print(bfs_beam_edges_path(G, src, dst))\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tmp = np.zeros((2 ,3))\n",
    "tmp[:, -1] = 2\n",
    "tmp[0, -1] == 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def calc_cosine_dist(p1, p2):\n",
    "    return np.dot(p1, p2) / (np.linalg.norm(p1) * np.linalg.norm(p2))\n",
    "\n",
    "def encode_edges(G, node, dst):\n",
    "    \n",
    "    ret = np.zeros((max_degree, 6))\n",
    "    ret[:, -1] = -1\n",
    "    \n",
    "    for idx, edge in enumerate(G.edges(node)):\n",
    "        u = edge[0]\n",
    "        v = edge[1]\n",
    "        \n",
    "        ret[idx][0] = G.get_edge_data(u, v)['centrality']\n",
    "        ret[idx][1] = G.get_edge_data(u, v)['weight']\n",
    "        \n",
    "        x_u = G.nodes[u]['x']\n",
    "        y_u = G.nodes[u]['y']\n",
    "        x_v = G.nodes[v]['x']\n",
    "        y_v = G.nodes[v]['y']\n",
    "        x_dst = G.nodes[dst]['x']\n",
    "        y_dst = G.nodes[dst]['y']\n",
    "        \n",
    "        ret[idx][2] = calc_cosine_dist((x_v-x_u, y_v-y_u), (x_dst-x_u, y_dst-y_u))\n",
    "        ret[idx][3] = x_v\n",
    "        ret[idx][4] = y_v\n",
    "        ret[idx][-1] = v\n",
    "        \n",
    "    return (ret, x_dst, y_dst)\n",
    "\n",
    "def generate_dataset(G):\n",
    "    \n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    for paths in nx.all_pairs_dijkstra_path(G):\n",
    "\n",
    "        print(\"src :%d\" % paths[0])\n",
    "        print(paths[1])\n",
    "        print('')\n",
    "        src = paths[0]\n",
    "\n",
    "        for dst, path in paths[1].items():\n",
    "\n",
    "            if dst == src:\n",
    "                continue\n",
    "\n",
    "            # X = cur_node + dst_node\n",
    "            # y = next_node (Dijkstra)\n",
    "\n",
    "            cur_node = src\n",
    "            for mid_node in path[1:]:\n",
    "\n",
    "                print('X:(%d, %d), y:(%d)' % (cur_node, dst, mid_node))\n",
    "                X.append(encode_edges(G, cur_node, dst))\n",
    "                y.append(mid_node)\n",
    "\n",
    "                cur_node = mid_node\n",
    "\n",
    "    #         print(cur_node, dst)\n",
    "            print(path)\n",
    "            print('')\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features, labels = generate_dataset(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def find_label_idx(feature, label):\n",
    "    \n",
    "    for idx, row in enumerate(feature):\n",
    "        if label == row[-1]:\n",
    "            break\n",
    "    \n",
    "    return idx\n",
    "\n",
    "def augment_dataset(X, y, augmentation_index=20):\n",
    "    \n",
    "    X_aug, y_aug = [], []\n",
    "    indices = np.arange(max_degree)\n",
    "    \n",
    "    for feature, label in zip(X, y):\n",
    "        for _ in range(augmentation_index):\n",
    "            np.random.shuffle(indices)\n",
    "            \n",
    "            feature_tmp = feature[0][indices]\n",
    "            label_tmp = find_label_idx(feature_tmp, label)\n",
    "            X_aug.append(np.append(feature_tmp[:, :-1].reshape(-1), [feature[1], feature[2]]))\n",
    "            y_aug.append(label_tmp)\n",
    "            \n",
    "    return np.array(X_aug), np.array(y_aug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "X, y = augment_dataset(features, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
