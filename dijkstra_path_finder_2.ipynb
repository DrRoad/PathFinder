{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Heads-Up\n",
    "\n",
    "In the notebook, we are working on generating\n",
    "graphs whose nodes have Euclidean coordinates;\n",
    "One thing to pay attention to is, in real world\n",
    "road network, nodes (intersections) that are close\n",
    "to each other geographically are more likely to be\n",
    "connected with edges (roads).\n",
    "\n",
    "Edge centrality will be added as part of the heuristic;\n",
    "meantime, cosine distance can also be a good heuristic;\n",
    "The geographical coordinates of nodes can be included."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions\n",
    "\n",
    "Does centrality really encode information about your destination?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Think with Analogy\n",
    "\n",
    "The model should not look back, only forward;\n",
    "The graph used for training is small and\n",
    "the trained model is used to infer nodes on\n",
    "similar sized graphs; perhaps training with\n",
    "big graphs will lead to better inference\n",
    "on smaller graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import numpy as np\n",
    "import mxnet as mx\n",
    "import logging\n",
    "from sklearn.metrics import accuracy_score\n",
    "from utils import plot_g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_NODE = 100\n",
    "WEIGHT_MIN = .0\n",
    "WEIGHT_MAX = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_euclidean_dist(G, tmp_node, node):\n",
    "    \n",
    "    p_1 = np.array([G.nodes[tmp_node]['x'], G.nodes[tmp_node]['y']])\n",
    "    p_2 = np.array([G.nodes[node]['x'], G.nodes[node]['y']])\n",
    "    return np.sqrt(np.sum((p_1 - p_2)**2))\n",
    "\n",
    "def generate_low_degree_g(node_size=100, min_out_degree=2, max_out_degree=4, weight_min=WEIGHT_MIN, weight_max=WEIGHT_MAX):\n",
    "    \n",
    "    G = nx.Graph()\n",
    "    \n",
    "    grid_size = 1000\n",
    "    euclidean_coords = np.linspace(0.01, 1.0, num=grid_size, endpoint=False)\n",
    "    coords_indices = list(range(grid_size))\n",
    "    \n",
    "    random.shuffle(coords_indices)\n",
    "    x_coords = euclidean_coords[coords_indices][:node_size]\n",
    "#     print(x_coords)\n",
    "    random.shuffle(coords_indices)\n",
    "    y_coords = euclidean_coords[coords_indices][:node_size]\n",
    "#     print(y_coords)\n",
    "    \n",
    "    # Add coordinates to nodes\n",
    "    for node, coord in enumerate(zip(x_coords, y_coords)):\n",
    "#         print(node, coord[0], coord[1])\n",
    "        G.add_node(node, x=coord[0], y=coord[1])\n",
    "    \n",
    "    for node in G.nodes:\n",
    "        \n",
    "        tmp_nodes = list(G.nodes)\n",
    "        tmp_nodes.remove(node)\n",
    "        node_dist = map(lambda tmp_node: (tmp_node, calc_euclidean_dist(G, tmp_node, node)), tmp_nodes)\n",
    "        node_dist = sorted(node_dist, key=lambda d:d[1])\n",
    "        \n",
    "        num_of_neighbors = random.randint(min_out_degree, max_out_degree)\n",
    "#         print(node, out_neighbors)\n",
    "        \n",
    "#         G.add_edges_from(map(lambda d:(node, d), out_neighbors))\n",
    "        \n",
    "        for tmp_node in node_dist:\n",
    "\n",
    "            if G.degree(tmp_node[0]) >= max_out_degree \\\n",
    "                or G.degree(node) >= num_of_neighbors:\n",
    "                # This node has maximum number of neighbors already\n",
    "                continue\n",
    "            \n",
    "            weight = random.uniform(weight_min, weight_max)\n",
    "            G.add_edge(node, tmp_node[0], weight=weight)\n",
    "    \n",
    "    # Add centrality to edges\n",
    "    edge_centrality = nx.edge_betweenness_centrality(G, \\\n",
    "                                                     k=G.number_of_nodes(), \\\n",
    "                                                     weight='weight')\n",
    "    assert len(edge_centrality) == G.number_of_edges()\n",
    "    \n",
    "    for edge_data in G.edges.data():\n",
    "        edge_data[2]['centrality'] = edge_centrality[(edge_data[0], edge_data[1])]\n",
    "        \n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = generate_low_degree_g(node_size=100)\n",
    "max_degree = max(G.degree, key=lambda d: d[1])[1]\n",
    "min_degree = min(G.degree, key=lambda d: d[1])[1]\n",
    "print(max_degree)\n",
    "print(min_degree)\n",
    "print(G.number_of_nodes())\n",
    "print(G.number_of_edges())\n",
    "plot_g(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_cosine_dist(p1, p2):\n",
    "    return np.dot(p1, p2) / (np.linalg.norm(p1) * np.linalg.norm(p2))\n",
    "\n",
    "def encode_edges(G, parent, node, dst):\n",
    "    \n",
    "    ret = np.zeros((max_degree, 7))\n",
    "    ret[:, -1] = -1\n",
    "    \n",
    "    x_dst = G.nodes[dst]['x']\n",
    "    y_dst = G.nodes[dst]['y']\n",
    "    \n",
    "    for idx, edge in enumerate(G.edges(node)):\n",
    "        u = edge[0]\n",
    "        v = edge[1]\n",
    "        \n",
    "#         Only look forward\n",
    "#         if v == parent:\n",
    "#             continue\n",
    "        \n",
    "        ret[idx][0] = G.get_edge_data(u, v)['centrality']\n",
    "        ret[idx][1] = G.get_edge_data(u, v)['weight']\n",
    "        \n",
    "        x_u = G.nodes[u]['x']\n",
    "        y_u = G.nodes[u]['y']\n",
    "        x_v = G.nodes[v]['x']\n",
    "        y_v = G.nodes[v]['y']\n",
    "\n",
    "        ret[idx][2] = calc_cosine_dist((x_v-x_u, y_v-y_u), (x_dst-x_u, y_dst-y_u))\n",
    "        ret[idx][3] = calc_euclidean_dist(G, v, dst)\n",
    "        ret[idx][4] = x_v\n",
    "        ret[idx][5] = y_v\n",
    "        ret[idx][-1] = v\n",
    "        \n",
    "    return (ret, x_dst, y_dst)\n",
    "\n",
    "def generate_dataset(G):\n",
    "    \n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    for paths in nx.all_pairs_dijkstra_path(G):\n",
    "\n",
    "        print(\"src :%d\" % paths[0])\n",
    "        print(paths[1])\n",
    "        print('')\n",
    "        src = paths[0]\n",
    "\n",
    "        for dst, path in paths[1].items():\n",
    "\n",
    "            if dst == src:\n",
    "                continue\n",
    "\n",
    "            # X = cur_node + dst_node\n",
    "            # y = next_node (Dijkstra)\n",
    "            parent_node = -1\n",
    "            cur_node = src\n",
    "            for mid_node in path[1:]:\n",
    "\n",
    "                print('parent:(%d), X:(%d, %d), y:(%d)' % (parent_node, cur_node, dst, mid_node))\n",
    "                X.append(encode_edges(G, parent_node, cur_node, dst))\n",
    "                y.append(mid_node)\n",
    "                \n",
    "                parent_node = cur_node\n",
    "                cur_node = mid_node\n",
    "\n",
    "    #         print(cur_node, dst)\n",
    "            print(path)\n",
    "            print('')\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features, labels = generate_dataset(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_label_idx(feature, label):\n",
    "    \n",
    "    for idx, row in enumerate(feature):\n",
    "        if label == row[-1]:\n",
    "            break\n",
    "    \n",
    "    return idx\n",
    "\n",
    "def one_hot_encode(idx, length=max_degree):\n",
    "    \n",
    "    ret = np.zeros(length)\n",
    "    ret[idx] = 1.0\n",
    "    return ret\n",
    "\n",
    "def augment_dataset(X, y, augmentation_index=20):\n",
    "    \n",
    "    X_aug, y_aug = [], []\n",
    "    indices = np.arange(max_degree)\n",
    "    \n",
    "    for feature, label in zip(X, y):\n",
    "        for _ in range(augmentation_index):\n",
    "            np.random.shuffle(indices)\n",
    "            \n",
    "            feature_tmp = feature[0][indices]\n",
    "            label_tmp = find_label_idx(feature_tmp, label)\n",
    "            X_aug.append(np.append(feature_tmp[:, :-1].reshape(-1), [feature[1], feature[2]]))\n",
    "            y_aug.append(label_tmp)\n",
    "#             y_aug.append(one_hot_encode(label_tmp))\n",
    "            \n",
    "    return np.array(X_aug), np.array(y_aug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = augment_dataset(features, labels, augmentation_index=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.85, test_size=.15, random_state=42)\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "train_iter = mx.io.NDArrayIter(X_train, y_train, batch_size, shuffle=True)\n",
    "val_iter = mx.io.NDArrayIter(X_test, y_test, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(context=mx.gpu()):\n",
    "    \n",
    "    data = mx.sym.var('data')\n",
    "    label = mx.sym.var('softmax_label')\n",
    "    \n",
    "    fc1  = mx.sym.FullyConnected(data=data, num_hidden=64)\n",
    "    fc1 = mx.sym.Activation(data=fc1, act_type=\"relu\")\n",
    "    \n",
    "    fc2  = mx.sym.FullyConnected(data=fc1, num_hidden=32)\n",
    "    fc2 = mx.sym.Activation(data=fc2, act_type=\"relu\")\n",
    "    \n",
    "    fc3  = mx.sym.FullyConnected(data=fc2, num_hidden=max_degree)\n",
    "    mlp  = mx.sym.SoftmaxOutput(data=fc3, label=label)\n",
    "    \n",
    "    return mx.mod.Module(symbol=mlp, context=context)\n",
    "\n",
    "def custom_acc(label, pred):\n",
    "    tmp_label = np.argmax(label, axis=1)\n",
    "    tmp_pred = np.argmax(pred, axis=1)\n",
    "    return accuracy_score(tmp_label, tmp_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.getLogger().setLevel(logging.DEBUG)  # logging to stdout\n",
    "model = build_model()\n",
    "model.bind(data_shapes=train_iter.provide_data, label_shapes=train_iter.provide_label)\n",
    "model.init_params()\n",
    "\n",
    "custom_metric = mx.metric.create(custom_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.fit(train_iter,  # train data\n",
    "              eval_data=val_iter,  # validation data\n",
    "              optimizer='adam',  # use SGD to train\n",
    "#               optimizer_params={'learning_rate':0.01, 'momentum': 0.9},\n",
    "              eval_metric='acc',  # report accuracy during training\n",
    "              batch_end_callback = mx.callback.Speedometer(batch_size, 500), # output progress for each 100 data batches\n",
    "              num_epoch=10)  # train for at most 10 dataset passes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc on training set 0.949833\n",
      "Acc on training set 0.950285\n"
     ]
    }
   ],
   "source": [
    "acc = mx.metric.Accuracy()\n",
    "train_iter = mx.io.NDArrayIter(X_train, y_train, batch_size=batch_size)\n",
    "model.score(train_iter, acc)\n",
    "print('Acc on training set %f' % acc.get()[1])\n",
    "\n",
    "acc = mx.metric.Accuracy()\n",
    "test_iter = mx.io.NDArrayIter(X_test, y_test, batch_size=batch_size)\n",
    "model.score(test_iter, acc)\n",
    "print('Acc on training set %f' % acc.get()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_infer_next_node(G, parent, cur_node, dst, model):\n",
    "#     print(\"In nn_infer_next_node\")\n",
    "    input_vec, x_dst, y_dst = encode_edges(G, parent, cur_node, dst)\n",
    "    \n",
    "    real_input_vec = np.append(input_vec[:, :-1].reshape(-1), [x_dst, y_dst])\n",
    "    pred = model.predict(mx.io.NDArrayIter(np.array([real_input_vec]), np.array([0]))).asnumpy()[0]\n",
    "    pred_idx = np.argmax(pred)\n",
    "    neighbor = int(input_vec[pred_idx][-1])\n",
    "    \n",
    "    if neighbor == -1:\n",
    "        print(\"Invalid prediction, randomizing next node\")\n",
    "        avaliable_neighbors = filter(lambda d: d >= 0, input_vec[:, -1])\n",
    "        neighbor = avaliable_neighbors[np.random.randint(0, len(avaliable_neighbors))]\n",
    "        \n",
    "    return (neighbor, G.get_edge_data(cur_node, neighbor)['weight'])\n",
    "\n",
    "def dijkstra_path_finder(G, src, dst, model=None):\n",
    "    \n",
    "    path = [src]\n",
    "    parent_node = -1\n",
    "    cur_node = src\n",
    "    total_weights = .0\n",
    "    \n",
    "    while True:\n",
    "        \n",
    "        if len(path) >= G.number_of_nodes():\n",
    "            return path, total_weights, False\n",
    "        \n",
    "        next_node, weight = nn_infer_next_node(G, parent_node, cur_node, dst, model)\n",
    "        total_weights += weight\n",
    "        path.append(next_node)\n",
    "        \n",
    "        if next_node == dst:\n",
    "            return path, total_weights, True\n",
    "        \n",
    "        parent_node = cur_node\n",
    "        cur_node = next_node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# G = generate_low_degree_g()\n",
    "\n",
    "found_cnt = 0\n",
    "opt_path_cnt = 0\n",
    "\n",
    "for src in G.nodes():\n",
    "    for dst in G.nodes():\n",
    "        \n",
    "        if src == dst:\n",
    "            continue\n",
    "        \n",
    "        nn_path, _, found = dijkstra_path_finder(G, src, dst, model=model)\n",
    "        path = nx.dijkstra_path(G, src, dst)\n",
    "        found_cnt = found_cnt + 1 if found else found_cnt\n",
    "        opt_path_cnt = opt_path_cnt + 1 if nn_path == path else opt_path_cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_nodes = G.number_of_nodes()\n",
    "num_total_path = num_of_nodes*(num_of_nodes-1)\n",
    "print('%d out of %d can find path: %f' % (found_cnt, num_total_path, float(found_cnt)/num_total_path))\n",
    "print('%d out of %d can find optimal path: %f' % (opt_path_cnt, found_cnt, float(opt_path_cnt)/found_cnt))\n",
    "print('%d out of %d all paths can find optimal paths: %f' % (opt_path_cnt, num_total_path, float(opt_path_cnt)/num_total_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_g(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
